📚 **Table of Contents**
- [Limitations of Ordinary Linear Regression]
  - [Multicollinearity](#1-multicollinearity)
  - [Example: Height in Inches vs. Height in Meters](#example-height-in-inches-vs-height-in-meters)
  - [Relationship](#relationship)
  - [Why This Is a Problem in Regression](#why-this-is-a-problem-in-regression)
  - [Correlation](#correlation)



# ⚠️ Limitations of Ordinary Linear Regression
### 1. **Multicollinearity**

* Multicollinearity occurs when two or more independent variables in a regression model are highly correlated with each other.
* When independent variables are **highly correlated**, it becomes hard to estimate the coefficients accurately.
* Leads to **unstable** and **unreliable** models.
* Multicollinearity denotes when independent variables in a linear regression equation are correlated.

## 📏 Example: Height in Inches vs. Height in Meters

| Person | Height (inches) $x_1$ | Height (meters) $x_2$ | Weight (kg) $y$ |
| ------ | --------------------- | --------------------- | --------------- |
| A      | 60                    | 1.524                 | 45              |
| B      | 65                    | 1.651                 | 55              |
| C      | 70                    | 1.778                 | 65              |
| D      | 72                    | 1.829                 | 70              |
| E      | 75                    | 1.905                 | 75              |

---

### 🔁 Relationship

$$
x_2 = x_1 \times 0.0254
$$

This means **Height in meters is linearly dependent** on Height in inches → **perfect multicollinearity**.

---

### 🧠 Why This Is a Problem in Regression

Suppose we try to model:

$$
y = \beta_0 + \beta_1 \cdot \text{Height\_inches} + \beta_2 \cdot \text{Height\_meters} + \epsilon
$$

The model will **fail to assign unique values** to $\beta_1$ and $\beta_2$, since both inputs contain the **same information**.

---

### 🧮 Correlation

$$
\text{Corr}(x_1, x_2) \approx 1
$$

Meaning they are **perfectly correlated**, causing:

* Inflated **standard errors**
* Unstable **coefficients**
* High **VIF values**

---

### 📌 2. High Variance When Number of Predictors is Large

* When the number of **predictors (features/input/independent variables)** is close to or exceeds the number of **observations (samples)**, the model becomes **unstable**.
* This results in a model that:

  * Fits the **training data too well**
  * Performs **poorly on test/unseen data**
* This is a classic case of **overfitting**, where the model captures noise rather than true patterns.



### 📌 3. **Overfitting: Model Fits Noise Instead of Signal**

* **Overfitting** happens when a model learns not only the **underlying patterns** in the data (signal), but also the **random fluctuations or errors** (noise).
* The model performs **very well on training data**, but **fails to generalize** to new or unseen data.

#### 🔍 Why It Happens:

* Too many predictors/features
* Very flexible models
* Small training datasets
* High variance in the data

#### 📉 Example:

* Imagine fitting a **complex curve** to just a few data points. It may pass through all the points (perfect accuracy), but perform poorly on future data.


### 📌 4. **Poor Generalization to Unseen Data**

* **Generalization** refers to how well a model performs on **new, unseen data** — not just the data it was trained on.
* **Ordinary Linear Regression** assumes:

  * A **linear relationship** between predictors and output
  * **Constant variance** of errors (homoscedasticity)
  * **Normally distributed** residuals
  * **Independence** of observations

#### ❌ When these assumptions are violated:

* The model may perform well on training data but **poorly on test data**.
* This leads to **low predictive accuracy** in real-world applications.

#### 💡 Example:

If the true relationship is **non-linear**, a linear model will **underfit**, missing key patterns — resulting in **poor generalization**.


| **Limitation**                                              | **Explanation**                                                                  | **Example**                                                                                     |
| ----------------------------------------------------------- | -------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- |
| **High Variance When Number of Predictors is Large**        | Model becomes unstable when there are too many predictors compared to data size. | Predicting house prices with 10 features but only 8 houses in the dataset.                      |
| **Multicollinearity Causes Unstable Coefficient Estimates** | Highly correlated predictors distort the influence of individual variables.      | Predicting salary using both `years of experience` and `age`, which are strongly correlated.    |
| **Overfitting: Fits Noise Instead of Signal**               | Model learns random noise rather than actual patterns in data.                   | A model gives perfect predictions on training data but fails completely on test data.           |
| **Poor Generalization to Unseen Data**                      | Model doesn't perform well on new or unseen data.                                | Linear regression used to predict non-linear relationships like stock prices or weather trends. |
