📚 **Table of Contents**
- [📘 Introduction to Regression](#-introduction-to-regression)
  - [🔍 What is Regression?](#-what-is-regression)
  - [💼 Use Cases](#-use-cases)

---

# 📘 Introduction to Regression

## 🔍 What is Regression?

Regression is a **statistical technique** used to:

* **Model and analyze** the relationship between a **dependent variable** (target) and one or more **independent variables** (features).
* **Predict a numeric outcome** based on given input variables.

---

## 💼 Use Cases

Regression is widely used in real-world scenarios such as:

* 🏠 **Predicting house prices**
* 📈 **Forecasting sales or stock returns**
* 🌡️ **Estimating temperature trends**
* 💰 **Estimating customer lifetime value**

---

## 🧠 Types of Regression

| Type                           | Description                                                                                                    |
| ------------------------------ | -------------------------------------------------------------------------------------------------------------- |
| **Simple Linear Regression**   | Models relationship between one independent variable and one dependent variable.                               |
| **Multiple Linear Regression** | Involves two or more independent variables to predict the dependent variable.                                  |
| **Polynomial Regression**      | Models a nonlinear relationship by using polynomial terms.                                                     |
| **Ridge Regression**           | A linear model with L2 regularization to prevent overfitting.                                                  |
| **Lasso Regression**           | Similar to Ridge but uses L1 regularization, which can shrink some coefficients to zero.                       |
| **ElasticNet**                 | Combines L1 and L2 regularization (Ridge + Lasso).                                                             |
| **Logistic Regression**        | Despite the name, used for **classification**, not regression; outputs probabilities for categorical outcomes. |

---

## **Simple Linear Regression:**
* **Description:** Models the relationship between **a single independent variable** and **a dependent variable** using a straight line.
* 1 input/feature 1 output
* 1 independent 1 dependent variable/feature
### 🧮 Equation of Simple Linear Regression

The general form:

$$
y = \beta_0 + \beta_1 x + \varepsilon
$$

* $x$ : Independent variable (input/feature)
* $y$ : Dependent variable (output/response)
* $\beta_0$ : Intercept (value of $y$ when $x = 0$)
* $\beta_1$ : Slope (change in $y$ for a unit change in $x$)
* $\varepsilon$ : Error term

---

## ✅ Assumptions of Simple Linear Regression

1. **Linearity**: The relationship between $x$ and $y$ is linear.
2. **Independence**: Observations are independent of each other.
3. **Homoscedasticity**: Constant variance of residuals across all levels of $x$.
4. **Normality of Residuals**: The residuals (errors) are normally distributed.

Absolutely! Here's a complete and visually intuitive explanation of the **Key Assumptions of Linear Regression**, focusing on **Linearity** — including **positive**, **negative**, and **non-linear (polynomial)** trends.

---

## ✅ Key Assumptions of Linear Regression

### 1. **Linearity**

The most fundamental assumption:

> There should be a **linear relationship** between the independent variable $x$ and the dependent variable $y$.

---

#### 📈 Types of Trends

#### ✅ **Positive Linear Relationship**

* As $x$ increases, $y$ also increases.
* **Suitable** for linear regression.

```
y
↑
|        *
|      *
|    *
|  *
|*
+----------------→ x
```

> Example: Hours studied vs. marks — more study, more marks.

---

#### ✅ **Negative Linear Relationship**

* As $x$ increases, $y$ decreases.
* **Suitable** for linear regression.

```
y
↑
|*
|  *
|    *
|      *
|        *
+----------------→ x
```

> Example: Hours spent gaming vs. sleep quality — more gaming, less sleep.

---

#### ❌ **Curvilinear (Non-Linear / Polynomial) Relationship**

* The relationship curves — may be **quadratic, cubic**, etc.
* **Not suitable** for simple linear regression.

```
y
↑
|     *       *
|   *           *
| *               *
|                   *
+----------------→ x
```

> Example: Age vs. income — income may rise in early career and fall later, forming a curve.

---


Would you like Python/Excel code to visualize this with real data?



#### 📈 **Examples of Polynomial Equations:**

#### ✅ Quadratic (2nd degree):

$$
y = \beta_0 + \beta_1 x + \beta_2 x^2 + \varepsilon
$$

* Forms a **U-shape** or **inverted U-shape** (parabola)

#### ✅ Cubic (3rd degree):

$$
y = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \varepsilon
$$

* Can model **S-shaped curves** or **multiple bends**

---


### 2. **Independence**: Observations are independent of each other.
$$
e_i = y_i - \hat{y}_i
$$

Since:

$$
\hat{y}_i = \beta_0 + \beta_1 x_i
$$

We get:

$$
e_i = y_i - (\beta_0 + \beta_1 x_i)
$$

---

### 📌 Where:

* $y_i$ = actual value of the response variable for observation $i$
* $\hat{y}_i$ = predicted value from the regression model
* $\beta_0$ = intercept
* $\beta_1$ = slope (coefficient of the predictor $x$)
* $e_i$ = **residual**, the difference between actual and predicted

## 🔧 Least Squares Estimation

We estimate $\beta_0$ and $\beta_1$ by minimizing the **sum of squared errors** (SSE):

$$
RSS = SSE = \sum_{i=1}^{n} e_i^2 
$$
$$
= \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$
$$
  = \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i))^2
$$
$$
  = \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2
$$

Where:

* Actual value : $y_i$
* Predicted value : $\hat{y}_i = \beta_0 + \beta_1 x_i$

### 🧮 Estimating the Coefficients


#### 1. **Intercept ($\beta_0$)**

$$
\beta_0 = \bar{y} - \beta_1 \bar{x}
$$

This is the value of $y$ when $x = 0$.


---
#### 2. **Slope ($\beta_1$)**

$$
\beta_1 = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}
$$

This measures how much $y$ changes for a unit change in $x$.

---


# [Problem]: Simple linear regression problem : 1
![alt text](assets/simple_linear_regression_math.png)

---

## 🔢 Step 1: Compute the Means

$$
\bar{x} = \frac{1 + 2 + 3 + 4 + 5}{5} = 3
$$

$$
\bar{y} = \frac{2 + 4 + 5 + 4 + 5}{5} = 4
$$

---

## 🧮 Step 2: Compute the Slope $\beta_1$

$$
\beta_1 = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}
$$

| $x_i$ | $y_i$ | $x_i - \bar{x}$ | $y_i - \bar{y}$ | $(x_i - \bar{x})(y_i - \bar{y})$ | $(x_i - \bar{x})^2$ |
| ----- | ----- | --------------- | --------------- | -------------------------------- | ------------------- |
| 1     | 2     | -2              | -2              | 4                                | 4                   |
| 2     | 4     | -1              | 0               | 0                                | 1                   |
| 3     | 5     | 0               | 1               | 0                                | 0                   |
| 4     | 4     | 1               | 0               | 0                                | 1                   |
| 5     | 5     | 2               | 1               | 2                                | 4                   |

$$
\sum (x_i - \bar{x})(y_i - \bar{y}) = 4 + 0 + 0 + 0 + 2 = 6
$$

$$
\sum (x_i - \bar{x})^2 = 4 + 1 + 0 + 1 + 4 = 10
$$

$$
\beta_1 = \frac{6}{10} = 0.6
$$

---

## 🧾 Step 3: Compute the Intercept $\beta_0$

$$
\beta_0 = \bar{y} - \beta_1 \bar{x} = 4 - (0.6)(3) = 2.2
$$

---

## ✅ Final Regression Equation

$$
\boxed{y = 2.2 + 0.6x}
$$

---

